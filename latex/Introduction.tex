\section{Introduction}\label{sec:introduction}

In this paper we take a look at \emph{Automatic Differentiation} through the eyes of \emph{Tensor} and \emph{Operational Calculus}. This work is best consumed as supplementary material for learning tensor and operational calculus by those already familiar with automatic differentiation. We first note the difference between the two concepts: \emph{automatic differentiation} is a name of a collection of techniques used for efficient computation of derivatives of differentiable programs, while \emph{operational calculus} is a technique that enables reasoning about analytic properties of differentiable programs through an algebra of higher order constructs. As rates of change (derivatives) are analytic properties, and automatic differentiation is used to compute them, we can explain automatic differentiation through the language of operational calculus. Note that this work will only cover as much tensor and operational calculus as needed for explanations of the implementation they are accompanying. This means that we will not delve into the many uses of operational calculus for differentiable programming that scope beyond automatic differentiation. As such, employment of operational calculus for reasoning about differentiable programs, ie. analysing fractional iterations, derivations of differentiable fixed point combinators, etc., will not be covered in this work (but can be found in \cite{OperationalCalculus}).

We provide a simple implementation of the memory space $\VV$ and its expansion to $\VV\otimes T(\VV^*)$, where $T(\VV^*)$ is the tensor algebra of its dual, serving by itself as an algebra of programs \cite[Definition~4.1]{OperationalCalculus}, and the operator $\tau_n$, that increases the order of a differentiable programming space \cite[Proposition~5.1]{OperationalCalculus}.
Source code can be found on GitHub \cite{dCpp}.


