\section{Introduction}\label{sec:introduction}

In this paper we take a look at \emph{Automatic Differentiation} through the eyes of \emph{Tensor} and \emph{Operational Calculus}. This work is best consumed as supplementary material for learning tensor and operational calculus by those already familiar with automatic differentiation. We first note the difference between the two concepts: \emph{automatic differentiation} is a name of a collection of techniques used for efficient computation of derivatives of differentiable programs, while \emph{operational calculus} is a technique that enables reasoning about analytic properties of differentiable programs through an algebra of higher order constructs. As rates of change (derivatives) are analytic properties, and automatic differentiation is used to compute them, we can explain automatic differentiation through the language of operational calculus.


To that effect we provide a simple implementation of the memory space $\VV$ and its expansion to $\VV\otimes T(\VV^*)$, where $T(\VV^*)$ is the tensor algebra of its dual, serving by itself as an algebra of programs \cite[Definition~4.1]{OperationalCalculus}, and the operator $\tau_n$, that increases the order of a differentiable programming space \cite[Proposition~5.1]{OperationalCalculus}.
Source code can be found on GitHub \cite{dCpp}.





